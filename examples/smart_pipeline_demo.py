#!/usr/bin/env python3
"""æ™ºèƒ½Pipelineå®Œæ•´ä½¿ç”¨ç¤ºä¾‹"""

import logging
import time
from src.python_test import (
    SmartLivePipeline,
    StreamConfig, 
    VLMConfig,
    Qwen2VLClient,
    ConsoleResultHandler
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

logger = logging.getLogger(__name__)

def demo_traditional_vs_smart():
    """ä¼ ç»Ÿæ¨¡å¼ vs æ™ºèƒ½æ¨¡å¼å¯¹æ¯”æ¼”ç¤º"""
    print("=" * 60)
    print("ğŸ¤– æ™ºèƒ½PipelineåŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    # åŸºç¡€é…ç½®
    stream_url = "rtmp://your-actual-stream-address/live/stream"  # è¯·æ›¿æ¢ä¸ºå®é™…æµåœ°å€
    stream_config = StreamConfig(url=stream_url, target_fps=1.0, max_queue_size=5)
    vlm_config = VLMConfig(device="cuda:0", max_new_tokens=256)
    vlm_client = Qwen2VLClient(vlm_config)
    
    print("æ¼”ç¤ºé…ç½®:")
    print(f"  æµåœ°å€: {stream_config.url}")
    print(f"  æŠ½å¸§é¢‘ç‡: {stream_config.target_fps} fps")
    print(f"  æ¨ç†è®¾å¤‡: {vlm_config.device}")
    print()
    
    # 1. ä¼ ç»Ÿæ¨¡å¼æ¼”ç¤º
    print("1ï¸âƒ£ ä¼ ç»Ÿæ¨¡å¼ (å›ºå®šé¢‘ç‡é‡‡æ ·)")
    print("-" * 40)
    traditional_pipeline = SmartLivePipeline(
        stream_config=stream_config,
        vlm_client=vlm_client,
        handlers=[ConsoleResultHandler()],
        enable_smart_sampling=False,
        enable_batch_processing=False
    )
    print("âœ… é…ç½®å®Œæˆ - å¤„ç†æ‰€æœ‰å¸§ï¼Œæ— ä¼˜åŒ–")
    
    # 2. æ™ºèƒ½é‡‡æ ·æ¨¡å¼æ¼”ç¤º
    print("\n2ï¸âƒ£ æ™ºèƒ½é‡‡æ ·æ¨¡å¼")
    print("-" * 40)
    smart_pipeline = SmartLivePipeline(
        stream_config=stream_config,
        vlm_client=vlm_client,
        handlers=[ConsoleResultHandler()],
        enable_smart_sampling=True,
        enable_batch_processing=False,
        motion_method="MOG2",
        ssim_threshold=0.85
    )
    print("âœ… é…ç½®å®Œæˆ - åŸºäºå†…å®¹å˜åŒ–æ™ºèƒ½é‡‡æ ·")
    print(f"   è¿åŠ¨æ£€æµ‹: {smart_pipeline.smart_sampler.motion_detector.method}")
    print(f"   å˜åŒ–é˜ˆå€¼: {smart_pipeline.smart_sampler.change_analyzer.ssim_threshold}")
    
    # 3. å®Œæ•´æ™ºèƒ½æ¨¡å¼æ¼”ç¤º
    print("\n3ï¸âƒ£ å®Œæ•´æ™ºèƒ½æ¨¡å¼")
    print("-" * 40)
    full_smart_pipeline = SmartLivePipeline(
        stream_config=stream_config,
        vlm_client=vlm_client,
        handlers=[ConsoleResultHandler()],
        enable_smart_sampling=True,
        enable_batch_processing=True,
        motion_method="MOG2",
        ssim_threshold=0.85,
        batch_buffer_size=5,
        batch_timeout=2.0
    )
    print("âœ… é…ç½®å®Œæˆ - æ™ºèƒ½é‡‡æ · + æ‰¹é‡å¤„ç†")
    print(f"   æ‰¹ç¼“å†²åŒº: {full_smart_pipeline.batch_processor.frame_buffer.max_size}")
    print(f"   æ‰¹è¶…æ—¶: {full_smart_pipeline.batch_processor.frame_buffer.max_wait_time}ç§’")
    
    print("\n" + "=" * 60)
    print("ğŸ¯ æ€§èƒ½ä¼˜åŒ–é¢„æœŸ:")
    print("=" * 60)
    print("ä¼ ç»Ÿæ¨¡å¼    : 100% å¸§å¤„ç†, 100% VLMè°ƒç”¨ (åŸºå‡†)")
    print("æ™ºèƒ½é‡‡æ ·    : 30-70% å¸§å¤„ç†, 30-70% VLMè°ƒç”¨ (é™ä½30-70%)")
    print("æ‰¹é‡å¤„ç†    : 100% å¸§å¤„ç†, 30-50% VLMè°ƒç”¨ (é™ä½50-70%)") 
    print("å®Œæ•´æ™ºèƒ½    : 30-70% å¸§å¤„ç†, 10-30% VLMè°ƒç”¨ (é™ä½70-90%)")
    print("=" * 60)
    
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. è¯·å°†ä¸Šé¢çš„æµåœ°å€æ›¿æ¢ä¸ºçœŸå®çš„RTMP/HTTPæµåœ°å€")
    print("2. é¦–æ¬¡ä½¿ç”¨å»ºè®®ç”¨CPUæ¨¡å¼æµ‹è¯•åŸºç¡€åŠŸèƒ½")
    print("3. ç”Ÿäº§ç¯å¢ƒæ¨èä½¿ç”¨GPUæ¨¡å¼è·å¾—æœ€ä½³æ€§èƒ½")
    print("4. å¯é€šè¿‡ --smart-sampling å’Œ --batch-processing å‚æ•°æ§åˆ¶åŠŸèƒ½å¼€å…³")
    
    # å®é™…è¿è¡Œç¤ºä¾‹ï¼ˆæ³¨é‡Šæ‰é¿å…æ„å¤–æ‰§è¡Œï¼‰
    # print("\nğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´æ™ºèƒ½æ¨¡å¼æ¼”ç¤º...")
    # try:
    #     full_smart_pipeline.run()
    # except KeyboardInterrupt:
    #     print("æ¼”ç¤ºå·²åœæ­¢")

if __name__ == "__main__":
    demo_traditional_vs_smart()
