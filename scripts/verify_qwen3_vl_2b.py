"""
Qwen3-VL-2B-Instruct æ¨¡å‹éªŒè¯è„šæœ¬

éªŒè¯å†…å®¹ï¼š
1. æ¨¡å‹åŠ è½½
2. å•å›¾ç†è§£
3. å¤šå›¾ç†è§£
4. è§†é¢‘ç†è§£ï¼ˆå¯é€‰ï¼‰
5. æ˜¾å­˜å ç”¨ç›‘æ§
"""

import os
import sys
import time
import torch
from PIL import Image

# æ·»åŠ é¡¹ç›®è·¯å¾„
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
sys.path.insert(0, PROJECT_ROOT)

# æ¨¡å‹é…ç½®
MODEL_ID = "Qwen/Qwen3-VL-2B-Instruct"
MODEL_LOCAL_PATH = os.path.join(PROJECT_ROOT, "models", "Qwen", "Qwen3-VL-2B-Instruct")


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–ç‰ˆæœ¬"""
    print("=" * 60)
    print("  ä¾èµ–æ£€æŸ¥")
    print("=" * 60)
    
    import transformers
    print(f"transformers ç‰ˆæœ¬: {transformers.__version__}")
    
    # Qwen3-VL éœ€è¦ transformers >= 4.57.0
    version_parts = transformers.__version__.split(".")
    major = int(version_parts[0])
    minor = int(version_parts[1]) if len(version_parts) > 1 else 0
    
    if major < 4 or (major == 4 and minor < 57):
        print(f"âš ï¸  è­¦å‘Š: Qwen3-VL éœ€è¦ transformers >= 4.57.0")
        print(f"   å½“å‰ç‰ˆæœ¬ {transformers.__version__} å¯èƒ½ä¸å…¼å®¹")
        print("   è¯·è¿è¡Œ: pip install 'transformers>=4.57.0'")
        return False
    
    print("âœ… transformers ç‰ˆæœ¬ç¬¦åˆè¦æ±‚")
    
    # æ£€æŸ¥ qwen-vl-utils
    try:
        import qwen_vl_utils
        print(f"âœ… qwen-vl-utils å·²å®‰è£…")
    except ImportError:
        print("âš ï¸  qwen-vl-utils æœªå®‰è£…")
        print("   è¯·è¿è¡Œ: pip install qwen-vl-utils")
        return False
    
    return True


def check_gpu():
    """æ£€æŸ¥ GPU çŠ¶æ€"""
    print("\n" + "=" * 60)
    print("  GPU çŠ¶æ€")
    print("=" * 60)
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        gpu_free = torch.cuda.memory_reserved(0) / (1024**3)
        
        print(f"GPU: {gpu_name}")
        print(f"æ€»æ˜¾å­˜: {gpu_memory:.2f} GB")
        print(f"å·²ç”¨æ˜¾å­˜: {gpu_free:.2f} GB")
        print(f"å¯ç”¨æ˜¾å­˜: {gpu_memory - gpu_free:.2f} GB")
        
        if gpu_memory < 6:
            print("âš ï¸  è­¦å‘Š: æ˜¾å­˜è¾ƒå°ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬")
        
        return True
    else:
        print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        return False


def download_model():
    """ä¸‹è½½æ¨¡å‹"""
    print("\n" + "=" * 60)
    print("  æ¨¡å‹ä¸‹è½½")
    print("=" * 60)
    
    # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(MODEL_LOCAL_PATH):
        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦æ–‡ä»¶
        required_files = ["config.json"]
        safetensors_file = os.path.join(MODEL_LOCAL_PATH, "model.safetensors")
        safetensors_index = os.path.join(MODEL_LOCAL_PATH, "model.safetensors.index.json")        
        # è‡³å°‘è¦æœ‰ config.json å’Œä¸€ä¸ªæ¨¡å‹æ–‡ä»¶
        if os.path.exists(os.path.join(MODEL_LOCAL_PATH, "config.json")) and \
           (os.path.exists(safetensors_file) or os.path.exists(safetensors_index)):
            print(f"âœ… æ¨¡å‹å·²å­˜åœ¨äº: {MODEL_LOCAL_PATH}")
            return MODEL_LOCAL_PATH
    
    print(f"æ­£åœ¨ä¸‹è½½æ¨¡å‹: {MODEL_ID}")
    print("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
    
    # åˆ›å»ºç›®æ ‡ç›®å½•
    os.makedirs(MODEL_LOCAL_PATH, exist_ok=True)
    
    try:
        from huggingface_hub import snapshot_download
        
        # å›½å†…é•œåƒåˆ—è¡¨
        mirrors = [
            {
                "name": "HuggingFace é•œåƒ (hf-mirror.com)",
                "endpoint": "https://hf-mirror.com",
                "env": "HF_ENDPOINT"
            },
            {
                "name": "ModelScope",
                "endpoint": None,
                "env": None,
                "use_modelscope": True
            },
            {
                "name": "HuggingFace å®˜æ–¹",
                "endpoint": None,
                "env": None
            }
        ]
        
        # ä¼˜å…ˆä½¿ç”¨é•œåƒ
        use_modelscope = os.environ.get("USE_MODELSCOPE", "false").lower() == "true"
        use_hf_mirror = os.environ.get("USE_HF_MIRROR", "true").lower() == "true"
                
        if use_modelscope:
            # ä½¿ç”¨ ModelScope
            print("ä½¿ç”¨ ModelScope ä¸‹è½½...")
            try:
                from modelscope import snapshot_download as ms_download
                model_path = ms_download(
                    "Qwen/Qwen3-VL-2B-Instruct",
                    cache_dir=os.path.join(PROJECT_ROOT, "models")
                )
                print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_path}")
                return model_path
            except ImportError:
                print("âš ï¸  ModelScope æœªå®‰è£…ï¼Œå°è¯•å…¶ä»–é•œåƒ...")
        
        # å°è¯• HuggingFace é•œåƒ
        for mirror in mirrors:
            if mirror.get("use_modelscope"):
                continue  # è·³è¿‡ ModelScopeï¼ˆå·²å¤„ç†ï¼‰
            
            try:
                print(f"\nå°è¯•: {mirror['name']}...")
                
                # è®¾ç½®é•œåƒç¯å¢ƒå˜é‡
                old_endpoint = os.environ.get("HF_ENDPOINT")
                if mirror["endpoint"]:
                    os.environ["HF_ENDPOINT"] = mirror["endpoint"]
                    print(f"  é•œåƒåœ°å€: {mirror['endpoint']}")
                
                model_path = snapshot_download(
                    MODEL_ID,
                    local_dir=MODEL_LOCAL_PATH,
                    etag_timeout=30,
                    resume_download=True
                )
                
                # æ¢å¤ç¯å¢ƒå˜é‡
                if old_endpoint:
                    os.environ["HF_ENDPOINT"] = old_endpoint
                elif "HF_ENDPOINT" in os.environ:
                    del os.environ["HF_ENDPOINT"]
                
                print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_path}")
                return model_path
                
            except Exception as e:
                print(f"  âŒ {mirror['name']} å¤±è´¥: {e}")
                continue
        
        print("\nâŒ æ‰€æœ‰é•œåƒéƒ½ä¸‹è½½å¤±è´¥")
        return None
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        print("\nå¤‡é€‰æ–¹æ¡ˆ:")
        print("1. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° models/Qwen/Qwen3-VL-2B-Instruct/")
        print("2. è®¾ç½® USE_MODELSCOPE=true ä½¿ç”¨ ModelScope")
        print("3. è®¾ç½® USE_HF_MIRROR=true ä½¿ç”¨å›½å†…é•œåƒ")
        return None


def load_model(model_path: str):
    """åŠ è½½æ¨¡å‹"""
    print("\n" + "=" * 60)
    print("  æ¨¡å‹åŠ è½½")
    print("=" * 60)
    
    from transformers import AutoModelForImageTextToText, AutoProcessor
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16 if device == "cuda" else torch.float32
    
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"è®¾å¤‡: {device}")
    print(f"ç²¾åº¦: {dtype}")
    
    load_kwargs = {
        "torch_dtype": dtype,
        "device_map": device if device == "cuda" else None,
        "low_cpu_mem_usage": True,
    }
    
    # å°è¯• Flash Attention
    if device == "cuda":
        try:
            load_kwargs["attn_implementation"] = "flash_attention_2"
            print("å°è¯•å¯ç”¨ Flash Attention 2...")
        except:
            pass
    
    start_time = time.time()
    
    try:
        model = AutoModelForImageTextToText.from_pretrained(model_path, **load_kwargs)
    except Exception as e:
        if "flash" in str(e).lower():
            print("Flash Attention ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›æœºåˆ¶")
            load_kwargs.pop("attn_implementation", None)
            model = AutoModelForImageTextToText.from_pretrained(model_path, **load_kwargs)
        else:
            raise
    
    if device == "cpu":
        model = model.to("cpu")
    
    model.eval()
    
    # åŠ è½½ processor
    processor = AutoProcessor.from_pretrained(
        model_path,
        min_pixels=256 * 28 * 28,
        max_pixels=512 * 28 * 28,  # é™åˆ¶åˆ†è¾¨ç‡èŠ‚çœæ˜¾å­˜
    )
    
    load_time = time.time() - start_time
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}s")
    
    # æ˜¾ç¤ºæ˜¾å­˜å ç”¨
    if device == "cuda":
        allocated = torch.cuda.memory_allocated(0) / (1024**3)
        reserved = torch.cuda.memory_reserved(0) / (1024**3)
        print(f"æ˜¾å­˜å ç”¨: {allocated:.2f} GB (å·²åˆ†é…), {reserved:.2f} GB (å·²é¢„ç•™)")
    
    return model, processor


def test_single_image(model, processor):
    """æµ‹è¯•å•å›¾ç†è§£"""
    print("\n" + "=" * 60)
    print("  æµ‹è¯• 1: å•å›¾ç†è§£")
    print("=" * 60)
    
    # ä½¿ç”¨æœ¬åœ°æµ‹è¯•å›¾ç‰‡æˆ–ç½‘ç»œå›¾ç‰‡
    test_image = "/apprun/jiankai/python_test/resource/IMG_20260108_113053_HC.jpeg"
    
    if not os.path.exists(test_image):
        test_image = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"
    
    print(f"æµ‹è¯•å›¾ç‰‡: {test_image}")
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": test_image},
                {"type": "text", "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬äººç‰©ã€åœºæ™¯å’Œæ´»åŠ¨ã€‚"},
            ],
        }
    ]
    
    # å‡†å¤‡è¾“å…¥
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_dict=True,
        return_tensors="pt"
    )
    inputs = inputs.to(model.device)
    
    # æ¨ç†
    start_time = time.time()
    with torch.inference_mode():
        generated_ids = model.generate(**inputs, max_new_tokens=256)
    
    inference_time = time.time() - start_time
    
    # è§£ç 
    generated_ids_trimmed = [
        out_ids[len(in_ids):]
        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]
    
    print(f"\næ¨ç†è€—æ—¶: {inference_time:.2f}s")
    print(f"\næ¨¡å‹å›ç­”:\n{output_text}")
    
    return True


def test_multi_image(model, processor):
    """æµ‹è¯•å¤šå›¾ç†è§£"""
    print("\n" + "=" * 60)
    print("  æµ‹è¯• 2: å¤šå›¾ç†è§£")
    print("=" * 60)
    
    # ä½¿ç”¨ç½‘ç»œå›¾ç‰‡
    test_images = [
        "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
        "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaini.jpg"
    ]
    
    print(f"æµ‹è¯•å›¾ç‰‡æ•°é‡: {len(test_images)}")
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": test_images[0]},
                {"type": "image", "image": test_images[1]},
                {"type": "text", "text": "è¯·æ¯”è¾ƒè¿™ä¸¤å¼ å›¾ç‰‡çš„å¼‚åŒç‚¹ã€‚"},
            ],
        }
    ]
    
    # å‡†å¤‡è¾“å…¥
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_dict=True,
        return_tensors="pt"
    )
    inputs = inputs.to(model.device)
    
    # æ¨ç†
    start_time = time.time()
    with torch.inference_mode():
        generated_ids = model.generate(**inputs, max_new_tokens=256)
    
    inference_time = time.time() - start_time
    
    # è§£ç 
    generated_ids_trimmed = [
        out_ids[len(in_ids):]
        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]
    
    print(f"\næ¨ç†è€—æ—¶: {inference_time:.2f}s")
    print(f"\næ¨¡å‹å›ç­”:\n{output_text}")
    
    return True


def test_video(model, processor):
    """æµ‹è¯•è§†é¢‘ç†è§£ï¼ˆå¯é€‰ï¼‰"""
    print("\n" + "=" * 60)
    print("  æµ‹è¯• 3: è§†é¢‘ç†è§£ï¼ˆå¯é€‰ï¼‰")
    print("=" * 60)
    
    # ä½¿ç”¨ç½‘ç»œè§†é¢‘
    test_video = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4"
    
    print(f"æµ‹è¯•è§†é¢‘: {test_video}")
    print("æ³¨æ„: è§†é¢‘ç†è§£éœ€è¦æ›´å¤šæ˜¾å­˜å’Œæ—¶é—´")
    
    try:
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "video", "video": test_video, "fps": 1.0},  # ä½å¸§ç‡èŠ‚çœæ˜¾å­˜
                    {"type": "text", "text": "è¯·æè¿°è¿™ä¸ªè§†é¢‘çš„å†…å®¹ã€‚"},
                ],
            }
        ]
        
        # å‡†å¤‡è¾“å…¥
        inputs = processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        inputs = inputs.to(model.device)
        
        # æ¨ç†
        start_time = time.time()
        with torch.inference_mode():
            generated_ids = model.generate(**inputs, max_new_tokens=256)
        
        inference_time = time.time() - start_time
        
        # è§£ç 
        generated_ids_trimmed = [
            out_ids[len(in_ids):]
            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        print(f"\næ¨ç†è€—æ—¶: {inference_time:.2f}s")
        print(f"\næ¨¡å‹å›ç­”:\n{output_text}")
        
        return True
        
    except Exception as e:
        print(f"âš ï¸  è§†é¢‘æµ‹è¯•è·³è¿‡: {e}")
        return False


def print_summary(results: dict):
    """æ‰“å°æµ‹è¯•æ‘˜è¦"""
    print("\n" + "=" * 60)
    print("  æµ‹è¯•æ‘˜è¦")
    print("=" * 60)
    
    total = len(results)
    passed = sum(1 for v in results.values() if v)
    
    for test, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{test}: {status}")
    
    print(f"\næ€»è®¡: {sum(1 for v in results.values() if v)}/{total} é€šè¿‡")
    
    if all(results.values()):
        print("\nğŸ‰ Qwen3-VL-2B éªŒè¯æˆåŠŸï¼æ¨¡å‹å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("   Qwen3-VL-2B-Instruct éªŒè¯è„šæœ¬")
    print("=" * 60)
    
    # 1. æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("\nâŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œè¯·å…ˆå®‰è£…å¿…è¦ä¾èµ–")
        return
    
    # 2. æ£€æŸ¥ GPU
    check_gpu()
    
    # 3. ä¸‹è½½æ¨¡å‹
    model_path = download_model()
    if model_path is None:
        print("\nâŒ æ¨¡å‹ä¸‹è½½å¤±è´¥")
        return
    
    # 4. åŠ è½½æ¨¡å‹
    try:
        model, processor = load_model(model_path)
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # 5. è¿è¡Œæµ‹è¯•
    results = {}
    
    try:
        results["å•å›¾ç†è§£"] = test_single_image(model, processor)
    except Exception as e:
        print(f"âŒ å•å›¾æµ‹è¯•å¤±è´¥: {e}")
        results["å•å›¾ç†è§£"] = False
    
    try:
        results["å¤šå›¾ç†è§£"] = test_multi_image(model, processor)
    except Exception as e:
        print(f"âŒ å¤šå›¾æµ‹è¯•å¤±è´¥: {e}")
        results["å¤šå›¾ç†è§£"] = False
    
    # è§†é¢‘æµ‹è¯•å¯é€‰
    try:
        results["è§†é¢‘ç†è§£"] = test_video(model, processor)
    except Exception as e:
        print(f"âš ï¸  è§†é¢‘æµ‹è¯•è·³è¿‡: {e}")
        results["è§†é¢‘ç†è§£"] = False
    
    # 6. æ‰“å°æ‘˜è¦
    print_summary(results)
    
    # 7. æ¸…ç†
    del model
    del processor
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("\næ˜¾å­˜å·²é‡Šæ”¾")


if __name__ == "__main__":
    main()
